#+TITLE: Cluster Autoscaler Code Analysis
#+AUTHOR: Code Analysis Session
#+DATE: [2024-12-19]
#+STARTUP: overview
#+OPTIONS: toc:2 num:t

* Table of Contents :TOC:
- [[#overview][Overview]]
- [[#runonce-function-analysis][RunOnce Function Analysis]]
  - [[#function-overview][Function Overview]]
  - [[#upcoming-node-handling][Upcoming Node Handling]]
  - [[#scale-up-prevention-mechanism][Scale-Up Prevention Mechanism]]
- [[#gpu-node-filtering-and-scheduling][GPU Node Filtering and Scheduling]]
  - [[#gpu-filtering-logic][GPU Filtering Logic]]
  - [[#impact-on-non-gpu-pods][Impact on Non-GPU Pods]]
- [[#nodes-with-unready-resources-complete-lifecycle][Nodes With Unready Resources - Complete Lifecycle]]
  - [[#overview-1][Overview]]
  - [[#problem-statement][Problem Statement]]
  - [[#filtering-logic][Filtering Logic]]
  - [[#timeout-configuration][Timeout Configuration]]
  - [[#lifecycle-and-state-transitions][Lifecycle and State Transitions]]
  - [[#what-happens-when-timeout-expires][What Happens When Timeout Expires]]
  - [[#fake-node-injection-for-unready-resources][Fake Node Injection for Unready Resources]]
  - [[#complete-flow-example][Complete Flow Example]]
  - [[#observability-and-diagnostics][Observability and Diagnostics]]
- [[#actionableclusterprocessor-safety-gate][ActionableClusterProcessor Safety Gate]]
  - [[#abort-conditions][Abort Conditions]]
  - [[#safety-benefits][Safety Benefits]]
- [[#clustersnapshot-concrete-types][ClusterSnapshot Concrete Types]]
  - [[#interface-and-implementation-hierarchy][Interface and Implementation Hierarchy]]
  - [[#configuration-and-usage][Configuration and Usage]]
- [[#fork-commit-revert-mechanism][Fork, Commit, Revert Mechanism]]
  - [[#core-concept][Core Concept]]
  - [[#implementation-architecture][Implementation Architecture]]
  - [[#practical-usage-examples][Practical Usage Examples]]
  - [[#performance-and-benefits][Performance and Benefits]]

* Overview

This document provides a detailed analysis of key components in the Kubernetes Cluster Autoscaler, focusing on:

1. How the =RunOnce= function handles nodes that are spinning up but not ready
2. GPU node filtering and its impact on scheduling
3. Complete lifecycle of nodes with unready custom resources (GPU, DRA)
4. Timeout configurations and behavior when resources don't become ready
5. The safety mechanisms that prevent autoscaling in unhealthy cluster states
6. ClusterSnapshot concrete types and implementation details
7. The fork/commit/revert transactional mechanism for safe simulations

* RunOnce Function Analysis

** Function Overview

The =RunOnce= function in =/home/hekumar/redhat/autoscaler/cluster-autoscaler/core/static_autoscaler.go= (lines 265-681) is the main orchestration function of the cluster autoscaler.

#+BEGIN_SRC go
func (a *StaticAutoscaler) RunOnce(currentTime time.Time) caerrors.AutoscalerError {
    // Main autoscaling loop operations
    a.cleanUpIfRequired()
    a.processorCallbacks.reset()
    a.clusterStateRegistry.PeriodicCleanup()
    // ... state collection and scaling decisions
}
#+END_SRC

** Upcoming Node Handling

*** Detection of Upcoming Nodes

The system identifies "upcoming nodes" through the =GetUpcomingNodes()= function:

#+BEGIN_SRC go
// Lines 450-456 in static_autoscaler.go
upcomingCounts, registeredUpcoming := a.clusterStateRegistry.GetUpcomingNodes()
// For each upcoming node we inject a placeholder node faked to appear ready
// into the cluster snapshot, so that we can pack unschedulable pods on
// them and not trigger another scale-up.
#+END_SRC

*** Upcoming Node Calculation Logic

From =clusterstate.go= lines 1015-1021:

#+BEGIN_SRC go
// newNodes is the number of nodes that should exist but don't yet
newNodes := ar.CurrentTarget - (len(readiness.Ready) + len(readiness.Unready) + len(readiness.LongUnregistered))
if newNodes <= 0 {
    continue
}
upcomingCounts[id] = newNodes
#+END_SRC

The calculation considers:
- *CurrentTarget*: Desired number of nodes after scale-up operations
- *Existing nodes*: Ready + Unready + LongUnregistered nodes
- *Difference*: Represents nodes that are "in flight" or being provisioned

** Scale-Up Prevention Mechanism

*** Cluster Snapshot Injection

The =addUpcomingNodesToClusterSnapshot= function (lines 684-713) creates *fake placeholder nodes*:

#+BEGIN_SRC go
func (a *StaticAutoscaler) addUpcomingNodesToClusterSnapshot(upcomingCounts map[string]int, nodeInfosForGroups map[string]*framework.NodeInfo) error {
    // Creates fake nodes that appear "ready" in scheduling simulation
    for nodeGroupName, upcomingNodeInfos := range upcomingNodeInfosPerNg {
        for _, upcomingNodeInfo := range upcomingNodeInfos {
            err := a.ClusterSnapshot.AddNodeInfo(upcomingNodeInfo)
            // Fake nodes allow unschedulable pods to be virtually placed
        }
    }
}
#+END_SRC

*** The Add-Then-Remove Pattern

The code follows a complex add-then-remove pattern that handles two types of upcoming nodes:

**** Problem: Two Categories of Upcoming Nodes

When the cluster autoscaler scales up, there are *two categories* of "upcoming nodes":

1. *Completely new nodes*: Requested from cloud provider but not yet registered in Kubernetes
2. *Registered but not ready nodes*: Already registered in Kubernetes but still in "NotStarted" state (not yet ready)

From the =GetUpcomingNodes()= function (lines 1021-1026 in =clusterstate.go=):
- =upcomingCounts= contains the *total count* of both types
- =registeredNodeNames= contains *only the names* of registered-but-not-ready nodes

**** Step 1: Add Fake Nodes for ALL Upcoming Nodes

#+BEGIN_SRC go
// Line 456 in static_autoscaler.go
err = a.addUpcomingNodesToClusterSnapshot(upcomingCounts, nodeInfosForGroups)
#+END_SRC

This adds *fake placeholder nodes* to the cluster snapshot for ALL upcoming nodes (both registered and unregistered). These fake nodes:
- Are marked as "ready" so pods can be scheduled on them
- Prevent triggering additional scale-ups when pods are already "scheduled" on upcoming capacity
- Use generated names like ="upcoming-0"=, ="upcoming-1"=, etc.

**** Step 2: Remove Real Registered Nodes to Prevent Conflicts

#+BEGIN_SRC go
// Lines 468-479 in static_autoscaler.go
allNodes = subtractNodesByName(allNodes, allRegisteredUpcoming)
// Remove the nodes from the snapshot as well so that the state is consistent
for _, notStartedNodeName := range allRegisteredUpcoming {
    err := a.ClusterSnapshot.RemoveNodeInfo(notStartedNodeName)
    if err != nil {
        klog.Errorf("Failed to remove NotStarted node %s from cluster snapshot: %v", notStartedNodeName, err)
        if !errors.Is(err, clustersnapshot.ErrNodeNotFound) {
            return caerrors.ToAutoscalerError(caerrors.InternalError, err)
        }
    }
}
#+END_SRC

**** Why This Pattern is Necessary

1. *Prevent Double Representation*: Without removal, registered-but-not-ready nodes would exist twice in the cluster snapshot:
   - Once as the real node (not ready, can't schedule pods)
   - Once as a fake node (ready, can schedule pods)

2. *Avoid Scale-Down Conflicts*: As the comment explains (lines 461-463):
   #+BEGIN_QUOTE
   "The actual registered nodes have to be filtered out of the all nodes list so that scale-down can't consider them as candidates. Otherwise, with aggressive scale-down settings, we could be removing the nodes before they have a chance to first become ready"
   #+END_QUOTE

3. *Consistent Scheduling State*: The cluster snapshot needs a consistent view where each upcoming node has exactly one representation - as a fake "ready" node that can accept pod scheduling.

**** End Result After Both Operations

After both add and remove operations:
- *Unregistered upcoming nodes*: Represented only by fake "ready" placeholders
- *Registered-but-not-ready nodes*: Real nodes removed, represented only by fake "ready" placeholders
- *Ready nodes*: Keep their real representation

This ensures the scheduler can make optimal decisions about pod placement while preventing premature scale-down of nodes that are still starting up.

*** Benefits of This Approach

1. *Prevents Over-Provisioning*: Scheduler doesn't request more nodes than needed
2. *Handles Cloud Provider Delays*: Accounts for time between node request and readiness
3. *Maintains Responsiveness*: Continues making scaling decisions while nodes provision
4. *Cleanup Capability*: Can remove nodes that fail to come up within expected timeframe
5. *Prevents Double Scheduling*: Avoids having both real and fake representations of the same node
6. *Scale-Down Protection*: Prevents aggressive scale-down from removing nodes before they become ready

* GPU Node Filtering and Scheduling

** GPU Filtering Logic

The =FilterOutNodesWithUnreadyResources= function in =gpu_processor.go= (lines 40-68) implements GPU readiness filtering:

#+BEGIN_SRC go
func (p *GpuCustomResourcesProcessor) FilterOutNodesWithUnreadyResources(context *context.AutoscalingContext, allNodes, readyNodes []*apiv1.Node, _ *drasnapshot.Snapshot) ([]*apiv1.Node, []*apiv1.Node) {
    for _, node := range readyNodes {
        _, hasGpuLabel := node.Labels[context.CloudProvider.GPULabel()]
        gpuAllocatable, hasGpuAllocatable := node.Status.Allocatable[gpu.ResourceNvidiaGPU]

        // Node has GPU label but no allocatable GPU resources
        if hasGpuLabel && (!hasGpuAllocatable || gpuAllocatable.IsZero()) {
            // Mark entire node as unready
            nodesWithUnreadyGpu[node.Name] = kubernetes.GetUnreadyNodeCopy(node, kubernetes.ResourceUnready)
        } else {
            newReadyNodes = append(newReadyNodes, node)
        }
    }
}
#+END_SRC

** Impact on Non-GPU Pods

*Unfortunately, non-GPU pods CANNOT be scheduled on these nodes* because:

1. *Node Marked as Unready*: The entire node is marked as "unready" (not just GPU resources)
2. *Removed from Ready Nodes*: Node is completely removed from the =readyNodes= list
3. *Scheduler Exclusion*: Only nodes in =readyNodes= list are considered for pod placement

*** Why This Design Choice?

- *GPU Driver Installation*: Nodes with GPU labels but no allocatable GPUs are likely installing drivers
- *Stability Concerns*: Node might be unstable during driver installation
- *Resource Prediction*: System can't reliably predict when GPU will become available
- *Consistent State*: Safer to wait until node is fully ready

* Nodes With Unready Resources - Complete Lifecycle

** Overview

The Cluster Autoscaler has a sophisticated mechanism for handling nodes that have registered in Kubernetes but whose custom resources (GPUs, DRA resources) are not yet ready. This addresses a known Kubernetes issue where allocatable resources can take up to 15 minutes to appear after a node registers as Ready.

** Problem Statement

*** The Root Issue (Kubernetes Issue #54959)

From =static_autoscaler.go= lines 1024-1029:

#+BEGIN_SRC go
// Handle GPU case - allocatable GPU may be equal to 0 up to 15 minutes after
// node registers as ready. See https://github.com/kubernetes/kubernetes/issues/54959
// Treat those nodes as unready until GPU actually becomes available and let
// our normal handling for booting up nodes deal with this.
// TODO: Remove this call when we handle dynamically provisioned resources.
allNodes, readyNodes = a.processors.CustomResourcesProcessor.FilterOutNodesWithUnreadyResources(a.AutoscalingContext, allNodes, readyNodes, draSnapshot)
#+END_SRC

*Key Problem*: Certain resources (GPUs, DirectX, DRA resources) can take significant time to show up in a node's allocatable resources even after the node itself has a Ready condition.

** Filtering Logic

*** GPU Resource Filtering

From =gpu_processor.go= lines 40-68:

#+BEGIN_SRC go
func (p *GpuCustomResourcesProcessor) FilterOutNodesWithUnreadyResources(context *context.AutoscalingContext, allNodes, readyNodes []*apiv1.Node, _ *drasnapshot.Snapshot) ([]*apiv1.Node, []*apiv1.Node) {
    newAllNodes := make([]*apiv1.Node, 0)
    newReadyNodes := make([]*apiv1.Node, 0)
    nodesWithUnreadyGpu := make(map[string]*apiv1.Node)

    for _, node := range readyNodes {
        _, hasGpuLabel := node.Labels[context.CloudProvider.GPULabel()]
        gpuAllocatable, hasGpuAllocatable := node.Status.Allocatable[gpu.ResourceNvidiaGPU]
        directXAllocatable, hasDirectXAllocatable := node.Status.Allocatable[gpu.ResourceDirectX]

        // Node has GPU label but no allocatable GPU resources
        if hasGpuLabel && ((!hasGpuAllocatable || gpuAllocatable.IsZero()) && (!hasDirectXAllocatable || directXAllocatable.IsZero())) {
            klog.V(3).Infof("Overriding status of node %v, which seems to have unready GPU", node.Name)
            nodesWithUnreadyGpu[node.Name] = kubernetes.GetUnreadyNodeCopy(node, kubernetes.ResourceUnready)
        } else {
            newReadyNodes = append(newReadyNodes, node)
        }
    }

    // Override any node with unready GPU with its "unready" copy
    for _, node := range allNodes {
        if newNode, found := nodesWithUnreadyGpu[node.Name]; found {
            newAllNodes = append(newAllNodes, newNode)
        } else {
            newAllNodes = append(newAllNodes, node)
        }
    }
    return newAllNodes, newReadyNodes
}
#+END_SRC

*** DRA Resource Filtering

From =dra_processor.go= lines 38-82:

#+BEGIN_SRC go
func (p *DraCustomResourcesProcessor) FilterOutNodesWithUnreadyResources(context *context.AutoscalingContext, allNodes, readyNodes []*apiv1.Node, draSnapshot *snapshot.Snapshot) ([]*apiv1.Node, []*apiv1.Node) {
    // Compares actual node ResourceSlices with expected ResourceSlices from node group template
    // If they don't match, mark node as unready

    for _, node := range readyNodes {
        ng, err := context.CloudProvider.NodeGroupForNode(node)
        nodeInfo, err := ng.TemplateNodeInfo()

        nodeResourcesSlices, _ := draSnapshot.NodeResourceSlices(node.Name)
        if isEqualResourceSlices(nodeResourcesSlices, nodeInfo.LocalResourceSlices) {
            newReadyNodes = append(newReadyNodes, node)
        } else {
            nodesWithUnreadyDraResources[node.Name] = kubernetes.GetUnreadyNodeCopy(node, kubernetes.ResourceUnready)
        }
    }

    // Override nodes in allNodes list with unready copies
    for _, node := range allNodes {
        if newNode, found := nodesWithUnreadyDraResources[node.Name]; found {
            newAllNodes = append(newAllNodes, newNode)
        } else {
            newAllNodes = append(newAllNodes, node)
        }
    }
    return newAllNodes, newReadyNodes
}
#+END_SRC

** GetUnreadyNodeCopy Implementation

From =ready.go= lines 133-150:

#+BEGIN_SRC go
// GetUnreadyNodeCopy create a copy of the given node and override its NodeReady condition to False
func GetUnreadyNodeCopy(node *apiv1.Node, reason NodeNotReadyReason) *apiv1.Node {
    newNode := node.DeepCopy()
    newReadyCondition := apiv1.NodeCondition{
        Type:               apiv1.NodeReady,
        Status:             apiv1.ConditionFalse,
        LastTransitionTime: node.CreationTimestamp,
        Reason:             string(reason),
    }
    newNodeConditions := []apiv1.NodeCondition{newReadyCondition}
    for _, condition := range newNode.Status.Conditions {
        if condition.Type != apiv1.NodeReady {
            newNodeConditions = append(newNodeConditions, condition)
        }
    }
    newNode.Status.Conditions = newNodeConditions
    return newNode
}
#+END_SRC

*Critical Actions*:
1. Deep copies the node
2. Overrides NodeReady condition to =False=
3. Sets reason to =ResourceUnready=
4. Replaces the original node in =allNodes= list
5. Removes the node from =readyNodes= list

** Timeout Configuration

*** Two Different Timeouts

There are two distinct timeouts that control when nodes transition from "starting" to "failed":

**** 1. MaxNodeStartupTime (Hardcoded)

From =clusterstate.go= lines 46-48:

#+BEGIN_SRC go
const (
    // MaxNodeStartupTime is the maximum time from the moment the node is registered to the time the node is ready.
    MaxNodeStartupTime = 15 * time.Minute
)
#+END_SRC

- *Type*: Hardcoded constant
- *Value*: 15 minutes
- *Applies to*: Registered nodes transitioning from NotStarted to Unready
- *Cannot be changed* without code modification

**** 2. MaxNodeProvisionTime (Configurable)

From =flags.go= line 126:

#+BEGIN_SRC go
maxNodeProvisionTime = flag.Duration("max-node-provision-time", 15*time.Minute,
    "The default maximum time CA waits for node to be provisioned - the value can be overridden per node group")
#+END_SRC

- *Type*: Command-line flag
- *Default Value*: 15 minutes
- *Applies to*: Unregistered nodes (cloud provider instances not yet in Kubernetes)
- *Configurable*: Can be set via =--max-node-provision-time= flag
- *Per-Node-Group Override*: Can be customized per node group via annotations

*** Timeout Configuration Examples

*Global Configuration*:
#+BEGIN_SRC bash
cluster-autoscaler --max-node-provision-time=20m0s
#+END_SRC

*Per-Node-Group Configuration* (Cluster API example):
#+BEGIN_SRC yaml
annotations:
  cluster.x-k8s.io/autoscaling-options-maxnodeprovisiontime: "20m0s"
#+END_SRC

** Lifecycle and State Transitions

*** State Transition Diagram

#+BEGIN_EXAMPLE
Cloud Provider Instance Created
         |
         v
    [Unregistered] ──────────────────┐
         |                           │ MaxNodeProvisionTime expires
         | Registers in K8s          │
         v                           v
    [NotStarted]              [LongUnregistered]
    (Node Ready=False,               |
     Reason=ResourceUnready)         v
         |                    Scale-up timeout
         |                    Node group backed off
         v
    MaxNodeStartupTime check
         |
         ├─> Still within 15min ──> [NotStarted] (keep waiting)
         |
         └─> Exceeds 15min ──────> [Unready]
                                          |
                                          v
                                    Available for scale-down
                                    Tracked in ResourceUnready metric
#+END_EXAMPLE

*** Readiness Classification Logic

From =clusterstate.go= lines 628-642:

#+BEGIN_SRC go
update := func(current Readiness, node *apiv1.Node, nr kube_util.NodeReadiness) Readiness {
    current.Registered = append(current.Registered, node.Name)
    if _, isDeleted := csr.deletedNodes[node.Name]; isDeleted {
        current.Deleted = append(current.Deleted, node.Name)
    } else if nr.Ready {
        current.Ready = append(current.Ready, node.Name)
    } else if node.CreationTimestamp.Time.Add(MaxNodeStartupTime).After(currentTime) {
        current.NotStarted = append(current.NotStarted, node.Name)
    } else {
        current.Unready = append(current.Unready, node.Name)
        if nr.Reason == kube_util.ResourceUnready {
            current.ResourceUnready = append(current.ResourceUnready, node.Name)
        }
    }
    return current
}
#+END_SRC

*Classification Logic*:
1. If node is marked for deletion → =Deleted=
2. Else if node has Ready condition → =Ready=
3. Else if within MaxNodeStartupTime (15 min) → =NotStarted=
4. Else → =Unready= (and possibly =ResourceUnready=)

** What Happens When Timeout Expires

*** For Registered Nodes (After MaxNodeStartupTime)

After =node.CreationTimestamp + 15 minutes=:

1. *State Transition*: Node moves from =NotStarted= to =Unready=
2. *Fake Node Removed*: Placeholder node removed from cluster snapshot
3. *Real Node Added Back*: Actual unready node added to scale-down candidates
4. *Metric Updated*: Node tracked in =ResourceUnready= metric for observability
5. *Scale-Down Eligible*: Node can now be considered for removal

*** For Unregistered Nodes (After MaxNodeProvisionTime)

From =clusterstate.go= lines 290-310:

#+BEGIN_SRC go
if scaleUpRequest.ExpectedAddTime.Before(currentTime) {
    klog.Warningf("Scale-up timed out for node group %v after %v",
        nodeGroupName, currentTime.Sub(scaleUpRequest.Time))
    csr.logRecorder.Eventf(apiv1.EventTypeWarning, "ScaleUpTimedOut",
        "Nodes added to group %s failed to register within %v",
        scaleUpRequest.NodeGroup.Id(), currentTime.Sub(scaleUpRequest.Time))

    csr.registerFailedScaleUpNoLock(scaleUpRequest.NodeGroup, metrics.Timeout, cloudprovider.InstanceErrorInfo{
        ErrorClass:   cloudprovider.OtherErrorClass,
        ErrorCode:    "timeout",
        ErrorMessage: fmt.Sprintf("Scale-up timed out for node group %v after %v", nodeGroupName, currentTime.Sub(scaleUpRequest.Time)),
    }, gpuResource, gpuType, currentTime)
    delete(csr.scaleUpRequests, nodeGroupName)
}
#+END_SRC

*Actions Taken*:
1. *Warning Logged*: ="Scale-up timed out for node group X"=
2. *Kubernetes Event*: Event with type =Warning= and reason =ScaleUpTimedOut=
3. *Failed Scale-Up Metric*: Registers metric with reason =Timeout=
4. *Node Group Backoff*: Node group is temporarily prevented from scaling up
5. *Request Removed*: Scale-up request is deleted from tracking
6. *Potential Retry*: CA may try a different node group if pods still pending

*** Backoff Behavior

From =clusterstate.go= line 350:

#+BEGIN_SRC go
func (csr *ClusterStateRegistry) registerFailedScaleUpNoLock(nodeGroup cloudprovider.NodeGroup, reason metrics.FailedScaleUpReason, errorInfo cloudprovider.InstanceErrorInfo, gpuResourceName, gpuType string, currentTime time.Time) {
    csr.scaleUpFailures[nodeGroup.Id()] = append(csr.scaleUpFailures[nodeGroup.Id()], ScaleUpFailure{NodeGroup: nodeGroup, Reason: reason, Time: currentTime})
    metrics.RegisterFailedScaleUp(reason, gpuResourceName, gpuType)
    csr.backoffNodeGroup(nodeGroup, errorInfo, currentTime)
}
#+END_SRC

The node group enters *exponential backoff*, preventing immediate retries and avoiding repeated failures.

** Fake Node Injection for Unready Resources

*** How Fake Nodes Are Created

When nodes with unready resources are in the =NotStarted= state:

1. *Counted as Upcoming*: From =clusterstate.go= lines 1013-1018:
   #+BEGIN_SRC go
   readiness := csr.perNodeGroupReadiness[id]
   ar := csr.acceptableRanges[id]
   // newNodes includes NotStarted nodes
   newNodes := ar.CurrentTarget - (len(readiness.Ready) + len(readiness.Unready) + len(readiness.LongUnregistered))
   upcomingCounts[id] = newNodes
   #+END_SRC

2. *Fake Nodes Injected*: From =static_autoscaler.go= lines 464-469:
   #+BEGIN_SRC go
   upcomingCounts, registeredUpcoming := a.clusterStateRegistry.GetUpcomingNodes()
   // For each upcoming node we inject a placeholder node faked to appear ready into the cluster snapshot,
   // so that we can pack unschedulable pods on them and not trigger another scale-up.
   err = a.addUpcomingNodesToClusterSnapshot(upcomingCounts, nodeInfosForGroups)
   #+END_SRC

3. *Real Node Removed*: From =static_autoscaler.go= lines 477-493:
   #+BEGIN_SRC go
   // Registered-but-not-ready nodes are removed from allNodes list
   allNodes = subtractNodesByName(allNodes, allRegisteredUpcoming)
   // Also removed from cluster snapshot
   for _, notStartedNodeName := range allRegisteredUpcoming {
       err := a.ClusterSnapshot.RemoveNodeInfo(notStartedNodeName)
   }
   #+END_SRC

*** Why Fake Nodes Are Necessary

From the comment in =static_autoscaler.go= lines 465-468:

#+BEGIN_QUOTE
"For each upcoming node we inject a placeholder node faked to appear ready into the cluster snapshot, so that we can pack unschedulable pods on them and not trigger another scale-up."
#+END_QUOTE

*Benefits*:
1. *Prevents Over-Provisioning*: Doesn't request more nodes when existing nodes are starting up
2. *Accurate Scheduling Simulation*: Pods can be "placed" on upcoming capacity
3. *Protects Starting Nodes*: From =static_autoscaler.go= lines 474-476:
   #+BEGIN_QUOTE
   "Otherwise, with aggressive scale-down settings, we could be removing the nodes before they have a chance to first become ready"
   #+END_QUOTE

*** Fake Node Template Source

Fake nodes are based on the node group's template:

#+BEGIN_SRC go
func (a *StaticAutoscaler) addUpcomingNodesToClusterSnapshot(upcomingCounts map[string]int, nodeInfosForGroups map[string]*framework.NodeInfo) error {
    // nodeInfosForGroups contains template NodeInfo for each node group
    // These templates have correct capacity, labels, taints from TemplateNodeInfo()
    for nodeGroupName, upcomingNodeInfos := range upcomingNodeInfosPerNg {
        for _, upcomingNodeInfo := range upcomingNodeInfos {
            err := a.ClusterSnapshot.AddNodeInfo(upcomingNodeInfo)
        }
    }
}
#+END_SRC

** Complete Flow Example

*** Example: GPU Node Startup

#+BEGIN_EXAMPLE
T+0min:  Cloud provider creates instance with GPU
         State: [Unregistered]
         Fake nodes: YES (1 fake node in snapshot)

T+2min:  Node registers in Kubernetes
         NodeReady: True (Kubernetes sees node as ready)
         GPU Allocatable: 0 (drivers still installing)
         FilterOutNodesWithUnreadyResources: Marks node as unready
         State: [NotStarted, ResourceUnready]
         Fake nodes: YES (still using fake node for scheduling)

T+5min:  GPU drivers still installing
         State: [NotStarted, ResourceUnready]
         Fake nodes: YES

T+8min:  GPU becomes allocatable
         GPU Allocatable: 1
         FilterOutNodesWithUnreadyResources: Passes node through
         State: [Ready]
         Fake nodes: NO (real node now used)
         Result: ✓ Success - node becomes ready

--- Alternative Timeline (Problem Case) ---

T+17min: GPU still not allocatable
         CreationTimestamp + 15min exceeded
         State: [Unready, ResourceUnready]
         Fake nodes: NO (fake node removed)
         Real unready node: Added back to snapshot
         Scale-down: Node now eligible for removal
         Result: ✗ Timeout - node marked as failed

T+20min: CA may scale down this unready node
         Scale-up request: Marked as timed out
         Node group: Enters backoff
         Retry: May try different node group
#+END_EXAMPLE

** Key Differences from Regular Node Startup

*** Regular Node (No Custom Resources)
- Registered → Ready → Immediately usable
- No filtering by =FilterOutNodesWithUnreadyResources=
- Straightforward lifecycle

*** Node With Custom Resources (GPU/DRA)
- Registered → Ready (K8s) → *Still filtered as unready* → Wait for resources → Finally ready
- Additional layer of readiness checking
- Accounts for resource discovery delays
- More complex state management

** Observability and Diagnostics

*** Metrics Available

1. *=cluster_autoscaler_nodes_count=* with label =state="notStarted"=
2. *=cluster_autoscaler_nodes_count=* with label =state="unready"=
3. *=cluster_autoscaler_nodes_count=* with label =state="resourceUnready"=
4. *=cluster_autoscaler_failed_scale_ups_total=* with reason =timeout=

*** Log Messages to Watch

#+BEGIN_SRC
# Node filtered due to unready GPU
"Overriding status of node %v, which seems to have unready GPU"

# Scale-up timeout warning
"Scale-up timed out for node group %v after %v"

# Kubernetes event
"Nodes added to group %s failed to register within %v"
#+END_SRC

** Readiness Structure

From =clusterstate.go= lines 599-622:

#+BEGIN_SRC go
// Readiness contains readiness information about a group of nodes.
type Readiness struct {
    Ready []string            // Names of ready nodes
    Unready []string          // Names of unready nodes that broke down after they started
    Deleted []string          // Names of nodes being deleted
    NotStarted []string       // Names of nodes not yet fully started
    Registered []string       // Names of all registered nodes
    LongUnregistered []string // Names of nodes that failed to register within limit
    Unregistered []string     // Names of nodes that haven't yet registered
    Time time.Time            // When readiness was measured
    ResourceUnready []string  // Names of nodes Unready due to missing resources
                             // (only for observability, doesn't influence CA behavior)
}
#+END_SRC

*Important Note*: =ResourceUnready= is a *subset* of =NotStarted= (if within 15 min) or =Unready= (if beyond 15 min). It's tracked separately only for observability and doesn't directly influence CA behavior.

* ActionableClusterProcessor Safety Gate

** Abort Conditions

The =ShouldAbort= method (lines 295-298 in =static_autoscaler.go=) checks three critical conditions:

#+BEGIN_SRC go
if abortLoop, err := a.processors.ActionableClusterProcessor.ShouldAbort(
    a.AutoscalingContext, allNodes, readyNodes, currentTime); abortLoop {
    return err
}
#+END_SRC

*** Implementation Details

From =actionable_cluster_processor.go= lines 49-65:

#+BEGIN_SRC go
func (e *EmptyClusterProcessor) ShouldAbort(context *context.AutoscalingContext, allNodes []*apiv1.Node, readyNodes []*apiv1.Node, currentTime time.Time) (bool, errors.AutoscalerError) {
    // 1. Scale-Up-From-Zero Check
    if context.AutoscalingOptions.ScaleUpFromZero {
        return false, nil  // Never abort if scale-up-from-zero enabled
    }

    // 2. No Nodes At All
    if len(allNodes) == 0 {
        OnEmptyCluster(context, "Cluster has no nodes.", true)
        return true, nil  // ABORT - completely empty cluster
    }

    // 3. No Ready Nodes
    if len(readyNodes) == 0 {
        // Wait up to 10 minutes after startup before considering error
        OnEmptyCluster(context, "Cluster has no ready nodes.",
                      currentTime.After(e.startTime.Add(e.nodesNotReadyAfterStartTimeout)))
        return true, nil  // ABORT - no ready nodes available
    }

    return false, nil  // Cluster is actionable
}
#+END_SRC

** Safety Benefits

*** What Happens When Aborted

When =abortLoop= is =true=, the =RunOnce= function:
- *Immediately returns* without performing scaling operations
- *Skips scale-up decisions* - no new nodes created
- *Skips scale-down decisions* - no existing nodes removed
- *Updates metrics* to indicate cluster is not safe to autoscale
- *Writes status* indicating cluster is "initializing"

*** Purpose and Benefits

1. *Prevents Invalid Operations*: Avoids scaling when cluster is in invalid state
2. *Startup Safety*: Handles autoscaler starting before nodes are ready
3. *Error Prevention*: Prevents cascading failures when cluster is broken
4. *Resource Conservation*: Avoids wasted API calls on unusable clusters
5. *Clear Diagnostics*: Provides logging and events when cluster is unhealthy

This acts as a *"cluster health check"* ensuring the autoscaler only operates on valid, actionable clusters.

* Key Takeaways

** Upcoming Node Management
- Cluster autoscaler tracks nodes being provisioned to prevent over-provisioning
- Fake placeholder nodes are injected into scheduling simulation
- System accounts for cloud provider delays in node provisioning

** GPU Resource Handling
- Nodes with unready GPU resources are marked entirely as unready
- Non-GPU pods cannot be scheduled on nodes with unready GPUs
- Conservative approach prioritizes stability over resource utilization

** Safety Mechanisms
- Multiple safety gates prevent autoscaling in unhealthy cluster states
- Grace periods handle startup scenarios gracefully
- Clear error reporting helps with diagnostics and troubleshooting

* ClusterSnapshot Concrete Types

** Interface and Implementation Hierarchy

The =ClusterSnapshot= object has the following concrete type structure:

*** Interface Type
#+BEGIN_SRC go
clustersnapshot.ClusterSnapshot
#+END_SRC

*** Concrete Implementation
The concrete type is *=*predicate.PredicateSnapshot=*, which is created in the main function:

#+BEGIN_SRC go
// From main.go lines 120-124
var snapshotStore clustersnapshot.ClusterSnapshotStore = store.NewDeltaSnapshotStore(autoscalingOptions.ClusterSnapshotParallelism)
opts := core.AutoscalerOptions{
    ClusterSnapshot: predicate.NewPredicateSnapshot(snapshotStore, fwHandle, autoscalingOptions.DynamicResourceAllocationEnabled),
    // ...
}
#+END_SRC

*** Complete Type Hierarchy

#+BEGIN_EXAMPLE
*predicate.PredicateSnapshot
├── ClusterSnapshotStore: *store.DeltaSnapshotStore  (by default)
├── pluginRunner: *SchedulerPluginRunner
└── draEnabled: bool
#+END_EXAMPLE

** Configuration and Usage

*** Breakdown of Components

1. *=PredicateSnapshot=* (=predicate_snapshot.go= lines 33-37):
   - Implements the =ClusterSnapshot= interface
   - Wraps a =ClusterSnapshotStore= with scheduler predicate checking capabilities
   - Handles DRA (Dynamic Resource Allocation) if enabled

2. *=DeltaSnapshotStore=* (=store/delta.go= line 53):
   - The default underlying storage implementation
   - Optimized for fork/commit operations used in scheduling simulations
   - More efficient than =BasicSnapshotStore= for autoscaler operations

3. *Alternative Storage*: =BasicSnapshotStore= (=store/basic.go= line 31):
   - Simple reference implementation
   - Used in tests and fallback scenarios
   - Less efficient but more straightforward

*** Key Methods Available

The =ClusterSnapshot= object provides these key methods used in the code:

- *=SetClusterState()=* - Resets and populates the snapshot with current cluster state
- *=AddNodeInfo()=* - Adds upcoming/fake nodes to the snapshot
- *=RemoveNodeInfo()=* - Removes nodes from the snapshot
- *=SchedulePod()=* - Simulates pod scheduling with predicate checking
- *=Fork()/Commit()/Revert()=* - Enables transactional operations for "what-if" scenarios

* Fork, Commit, Revert Mechanism

** Core Concept

The fork, commit, and revert mechanism provides *transactional operations* on the cluster snapshot, allowing for safe "what-if" simulations without affecting the main cluster state.

This mechanism implements a *layered state management system* where:
- *Fork()* creates a new layer for temporary modifications
- *Commit()* merges changes from the current layer to the base layer
- *Revert()* discards the current layer and returns to the previous state

** Implementation Architecture

*** Interface Definition (clustersnapshot.go lines 98-104)

#+BEGIN_SRC go
// Fork creates a fork of snapshot state. All modifications can later be reverted to moment of forking via Revert().
Fork()
// Revert reverts snapshot state to moment of forking.
Revert()
// Commit commits changes done after forking.
Commit() error
#+END_SRC

*** DeltaSnapshotStore Implementation (delta.go lines 534-558)

#+BEGIN_SRC go
// Fork creates a fork of snapshot state
// Time: O(1)
func (snapshot *DeltaSnapshotStore) Fork() {
    snapshot.data = snapshot.data.fork()
    snapshot.draSnapshot.Fork()
}

// Revert reverts snapshot state to moment of forking
// Time: O(1)
func (snapshot *DeltaSnapshotStore) Revert() {
    if snapshot.data.baseData != nil {
        snapshot.data = snapshot.data.baseData
    }
    snapshot.draSnapshot.Revert()
}

// Commit commits changes done after forking
// Time: O(n), where n = size of delta
func (snapshot *DeltaSnapshotStore) Commit() error {
    newData, err := snapshot.data.commit()
    if err != nil {
        return err
    }
    snapshot.data = newData
    snapshot.draSnapshot.Commit()
    return nil
}
#+END_SRC

*** Internal Data Structure (delta.go lines 62-73)

#+BEGIN_SRC go
type internalDeltaSnapshotData struct {
    baseData *internalDeltaSnapshotData  // Pointer to parent layer

    addedNodeInfoMap    map[string]*schedulerframework.NodeInfo
    modifiedNodeInfoMap map[string]*schedulerframework.NodeInfo
    deletedNodeInfos    map[string]bool

    // Cached lists and indexes
    nodeInfoList                     []*schedulerframework.NodeInfo
    havePodsWithAffinity             []*schedulerframework.NodeInfo
    havePodsWithRequiredAntiAffinity []*schedulerframework.NodeInfo
    pvcNamespaceMap                  map[string]int
}
#+END_SRC

** Practical Usage Examples

*** Binpacking Estimator (binpacking_estimator.go lines 108-111)

#+BEGIN_SRC go
e.clusterSnapshot.Fork()
defer func() {
    e.clusterSnapshot.Revert()  // Always revert - this is simulation only
}()

// Simulate scheduling pods on new nodes
// All changes are automatically reverted when function exits
#+END_SRC

*** Scale-Up Orchestrator (orchestrator.go lines 601-602)

#+BEGIN_SRC go
o.autoscalingContext.ClusterSnapshot.Fork()
defer o.autoscalingContext.ClusterSnapshot.Revert()

// Add test node to snapshot for simulation
if err := o.autoscalingContext.ClusterSnapshot.AddNodeInfo(nodeInfo); err != nil {
    // Handle error
}
// Test if pods can be scheduled on this node
// Changes are reverted when function exits
#+END_SRC

*** WithForkedSnapshot Helper (clustersnapshot.go lines 112-129)

#+BEGIN_SRC go
func WithForkedSnapshot(snapshot ClusterSnapshot, f func() (bool, error)) (error, error) {
    var commit bool
    var err, cleanupErr error
    snapshot.Fork()
    defer func() {
        if commit {
            cleanupErr = snapshot.Commit()
            if cleanupErr != nil {
                klog.Errorf("Got error when calling ClusterSnapshot.Commit(), will try to revert; %v", cleanupErr)
            }
        }
        if !commit || cleanupErr != nil {
            snapshot.Revert()  // Fallback to revert
        }
    }()
    commit, err = f()  // Function returns whether to commit or revert
    return err, cleanupErr
}
#+END_SRC

** Performance and Benefits

*** Key Benefits

1. *Safe Simulations*: Test scheduling scenarios without affecting real cluster state
2. *Performance Optimization*:
   - *O(1) Fork*: No data copying, just pointer manipulation
   - *Lazy evaluation*: Only track changes, not full state
   - *Efficient rollback*: Simply discard the delta layer
3. *Transactional Semantics*:
   - *Atomic operations*: Either all changes are applied or none
   - *Error recovery*: Automatic revert on failures
   - *Nested transactions*: Multiple fork levels supported
4. *Memory Efficiency*:
   - *Copy-on-write semantics*: Only modified data is duplicated
   - *Shared base state*: Multiple forks can share the same base
   - *Garbage collection friendly*: Discarded layers are automatically cleaned up

*** Use Cases in Cluster Autoscaler

1. *Pod Scheduling Simulation*: Test if pods can fit on existing or new nodes
2. *Scale-Up Estimation*: Calculate how many nodes are needed for pending pods
3. *Scale-Down Safety*: Verify that removing nodes won't break scheduling
4. *Resource Planning*: Evaluate different scaling scenarios
5. *Predicate Testing*: Check scheduling constraints without side effects

* Key Takeaways

** Upcoming Node Management
- Cluster autoscaler tracks nodes being provisioned to prevent over-provisioning
- Fake placeholder nodes are injected into scheduling simulation
- System accounts for cloud provider delays in node provisioning

** GPU and Custom Resource Handling
- Nodes with unready GPU resources are marked entirely as unready
- Non-GPU pods cannot be scheduled on nodes with unready GPUs
- Conservative approach prioritizes stability over resource utilization

** Unready Resources Lifecycle
- Two timeout configurations: MaxNodeStartupTime (15 min, hardcoded) and MaxNodeProvisionTime (15 min default, configurable)
- Nodes with unready resources are filtered out even if Kubernetes marks them Ready
- Fake nodes represent these nodes in cluster snapshot during startup period
- After timeout, nodes transition to Unready state and become eligible for scale-down
- Scale-up failures trigger node group backoff to prevent repeated failures

** Safety Mechanisms
- Multiple safety gates prevent autoscaling in unhealthy cluster states
- Grace periods handle startup scenarios gracefully
- Clear error reporting helps with diagnostics and troubleshooting

** ClusterSnapshot Architecture
- Uses layered PredicateSnapshot wrapping DeltaSnapshotStore for efficiency
- Provides both low-level storage and high-level predicate checking
- Configurable with different storage backends and DRA support

** Transactional Operations
- Fork/commit/revert mechanism enables safe "what-if" simulations
- O(1) fork operations with copy-on-write semantics
- Critical for making intelligent scaling decisions without side effects

* DRA/CSI Circular Dependency Problem Analysis

** Problem Overview

Based on the merged DRA solution ([PR #8109](https://github.com/kubernetes/autoscaler/pull/8109)) and proposed CSI solutions, there's a significant architectural issue: a circular dependency between readiness logic and TemplateNodeInfoProvider that breaks the backup mechanism for cloud providers.

** Current Flow and the Circular Dependency

*** Normal RunOnce Flow

From =static_autoscaler.go=:

#+BEGIN_SRC go
// RunOnce iterates over node groups and scales them up/down if necessary
func (a *StaticAutoscaler) RunOnce(currentTime time.Time) caerrors.AutoscalerError {
    // 1. Get and filter nodes
    allNodes, readyNodes, typedErr := a.obtainNodeLists(draSnapshot)  // Line 302

    // 2. Generate templates using filtered ready nodes
    nodeInfosForGroups, autoscalerError := a.processors.TemplateNodeInfoProvider.Process(
        autoscalingContext, readyNodes, daemonsets, a.taintConfig, currentTime)  // Line 369
}
#+END_SRC

*** The Circular Dependency

#+BEGIN_EXAMPLE
┌─ obtainNodeLists() needs TemplateNodeInfo() for readiness filtering
│  ├─ calls FilterOutNodesWithUnreadyResources()
│  └─ DRA processor calls ng.TemplateNodeInfo() (line 59 in dra_processor.go)
│
├─ TemplateNodeInfoProvider.Process() needs readyNodes from obtainNodeLists()
│  ├─ Depends on readiness logic running first
│  └─ Uses readyNodes to create final templates
│
└─ Within a single CA loop: TemplateNodeInfoProvider can't access its own output
#+END_EXAMPLE

** The Backup Mechanism That Gets Broken

*** How the Backup Mechanism Works

From =mixed_nodeinfos_processor.go= lines 106-149:

#+BEGIN_SRC go
// MixedTemplateNodeInfoProvider backup mechanism
func (p *MixedTemplateNodeInfoProvider) Process(...) {
    // Step 1: Try to use real nodes first (PREFERRED)
    for _, node := range nodes {
        if !isNodeGoodTemplateCandidate(node, now) {
            continue
        }
        // Use real node info - has complete resource information
        templateNodeInfo, caErr := simulator.SanitizedTemplateNodeInfoFromNodeInfo(nodeInfo, id, ...)
        result[id] = templateNodeInfo
    }

    // Step 2: Try cache from previous real nodes
    if cacheItem, found := p.nodeInfoCache[id]; found && !p.isCacheItemExpired(cacheItem.added) {
        result[id] = cacheItem.NodeInfo.DeepCopy()
        continue
    }

    // Step 3: LAST RESORT - Use synthetic template from cloud provider
    nodeInfo, err := simulator.SanitizedTemplateNodeInfoFromNodeGroup(nodeGroup, ...)
    if err == cloudprovider.ErrNotImplemented {
        // Many providers don't implement this completely
        continue
    }
    result[id] = nodeInfo
}
#+END_SRC

*** Why This Backup is Crucial

1. *Real nodes* have actual resource information (GPU, DRA, CSI resources)
2. *Synthetic templates* from =TemplateNodeInfo()= may lack this information
3. *Cache* preserves real node information even after nodes are deleted

*** Scale-from-Zero vs. Backup Scenarios

*Normal Scenario (CA works with incomplete TemplateNodeInfo)*:
- Cloud provider's =TemplateNodeInfo()= might not predict DRA/CSI resources
- **Backup works**: Keep at least 1 node in NodeGroup
- TemplateNodeInfoProvider uses real node → Complete resource info → ✅ Success

*DRA/CSI Readiness Logic Breaks This*:
#+BEGIN_EXAMPLE
Before: Real Node → TemplateNodeInfoProvider → Uses real node info → ✅ Complete

After:  Real Node → Readiness logic calls TemplateNodeInfo() → Incomplete template →
        Node marked unready → TemplateNodeInfoProvider can't use real node → ❌ Broken
#+END_EXAMPLE

** Detailed Problem Analysis

*** DRA Implementation Creates the Issue

From =dra_processor.go= lines 59-67:

#+BEGIN_SRC go
// This creates the circular dependency!
nodeInfo, err := ng.TemplateNodeInfo()  // Calls cloud provider's synthetic template
if err != nil {
    // Falls back to keeping node ready, but this defeats the purpose
    newReadyNodes = append(newReadyNodes, node)
    continue
}

// Compare synthetic template resources with real node resources
nodeResourcesSlices, _ := draSnapshot.NodeResourceSlices(node.Name)
if isEqualResourceSlices(nodeResourcesSlices, nodeInfo.LocalResourceSlices) {
    newReadyNodes = append(newReadyNodes, node)
} else {
    // Mark node as unready based on incomplete template!
    nodesWithUnreadyDraResources[node.Name] = kubernetes.GetUnreadyNodeCopy(node, kubernetes.ResourceUnready)
}
#+END_SRC

*** Integration Gap

*The core problem*: =TemplateNodeInfo()= for most cloud providers is **not integrated** with DRA/CSI:

1. Cloud providers implement =TemplateNodeInfo()= with basic resources (CPU, memory)
2. DRA/CSI resources are discovered dynamically after node startup
3. Readiness logic compares dynamic reality vs. incomplete synthetic template
4. Result: Nodes marked unready based on incomplete information

*** Lost Backup Mechanism

**Before DRA readiness logic**:
#+BEGIN_EXAMPLE
Node Group with 1 existing node:
├─ TemplateNodeInfoProvider finds existing ready node
├─ Creates template from real node (complete DRA/CSI resources)
├─ New nodes compared against complete template
└─ ✅ Works even if TemplateNodeInfo() incomplete
#+END_EXAMPLE

**After DRA readiness logic**:
#+BEGIN_EXAMPLE
Node Group with 1 existing node:
├─ Readiness logic calls incomplete TemplateNodeInfo()
├─ Real node marked unready (doesn't match incomplete template)
├─ TemplateNodeInfoProvider sees no ready nodes
├─ Falls back to same incomplete TemplateNodeInfo()
└─ ❌ Backup mechanism lost
#+END_EXAMPLE

** The Proper Solution: Template Extraction and Persistence

*** Architecture Overview

The solution requires **major refactor** to break the circular dependency:

#+BEGIN_EXAMPLE
Current (Broken):
Loop N: obtainNodeLists() → TemplateNodeInfoProvider.Process() → templates (local variable)
         ↑__________________________|

Fixed (With Persistence):
Loop N-1: TemplateNodeInfoProvider.Process() → TemplateRepository.SetTemplates()
Loop N:   TemplateRepository.GetTemplates() → obtainNodeLists() → TemplateNodeInfoProvider.Process()
Loop N+1: Uses templates from Loop N
#+END_EXAMPLE

*** New Component: TemplateRepository

#+BEGIN_SRC go
type TemplateRepository interface {
    // Get templates computed in previous loop
    GetTemplates() map[string]*framework.NodeInfo

    // Store templates for next loop
    SetTemplates(templates map[string]*framework.NodeInfo)

    // Clear expired templates
    Cleanup()
}

type InMemoryTemplateRepository struct {
    templates map[string]*framework.NodeInfo
    ttl       time.Duration
    mutex     sync.RWMutex
}
#+END_SRC

*** Modified RunOnce Structure

#+BEGIN_SRC go
func (a *StaticAutoscaler) RunOnce(currentTime time.Time) error {
    // 1. Get templates from previous loop (or bootstrap templates)
    previousTemplates := a.templateRepository.GetTemplates()

    // 2. Use previous templates for readiness logic (breaks circular dependency)
    allNodes, readyNodes := a.obtainNodeListsWithTemplates(draSnapshot, previousTemplates)

    // 3. Compute fresh templates using current readyNodes
    currentTemplates := a.processors.TemplateNodeInfoProvider.Process(...)

    // 4. Store current templates for next loop
    a.templateRepository.SetTemplates(currentTemplates)

    // 5. Continue with scaling logic using currentTemplates
    // ...
}
#+END_SRC

*** Modified Readiness Logic

#+BEGIN_SRC go
func (p *DraCustomResourcesProcessor) FilterOutNodesWithUnreadyResources(
    context *context.AutoscalingContext,
    allNodes, readyNodes []*apiv1.Node,
    draSnapshot *snapshot.Snapshot,
    templates map[string]*framework.NodeInfo, // ← New parameter from previous loop
) ([]*apiv1.Node, []*apiv1.Node) {

    for _, node := range readyNodes {
        ng, _ := context.CloudProvider.NodeGroupForNode(node)

        // Use template from previous loop instead of TemplateNodeInfo()
        var nodeInfo *framework.NodeInfo
        if template, exists := templates[ng.Id()]; exists {
            nodeInfo = template  // Use complete template from real nodes
        } else {
            // Fallback to TemplateNodeInfo() only if no previous template
            nodeInfo, _ = ng.TemplateNodeInfo()
        }

        // Compare with template that includes real resource information
        nodeResourcesSlices, _ := draSnapshot.NodeResourceSlices(node.Name)
        if isEqualResourceSlices(nodeResourcesSlices, nodeInfo.LocalResourceSlices) {
            newReadyNodes = append(newReadyNodes, node)
        } else {
            nodesWithUnreadyDraResources[node.Name] = kubernetes.GetUnreadyNodeCopy(node, kubernetes.ResourceUnready)
        }
    }
    return newAllNodes, newReadyNodes
}
#+END_SRC

*** Bootstrap Strategy

*Challenge*: First loop has no previous templates.

*Solution*: Bootstrap with cloud provider templates, then improve over time:

#+BEGIN_SRC go
func (r *InMemoryTemplateRepository) GetTemplates() map[string]*framework.NodeInfo {
    r.mutex.RLock()
    defer r.mutex.RUnlock()

    if len(r.templates) == 0 {
        // Bootstrap: Use cloud provider templates for first loop
        return r.bootstrapFromCloudProvider()
    }

    return r.copyTemplates()
}
#+END_SRC

** Benefits of the Solution

*** Fixes Core Problems

1. **Breaks Circular Dependency**: Readiness logic uses templates from previous loop
2. **Restores Backup Mechanism**: Real nodes can contribute to template generation
3. **Progressive Improvement**: Templates improve over time as real nodes provide data
4. **Cloud Provider Compatibility**: Works regardless of TemplateNodeInfo() completeness

*** Maintains Existing Behavior

1. **Scale-from-Zero**: Still works with cloud provider templates initially
2. **Timeout Handling**: No changes to existing timeout mechanisms
3. **Metrics/Observability**: All existing metrics continue to work
4. **Error Handling**: Robust fallback to cloud provider templates

*** Performance Considerations

1. **Memory**: Minimal overhead - just template storage between loops
2. **CPU**: Template copying overhead, but only between loops
3. **Latency**: No impact on loop execution time
4. **Complexity**: Contained within new TemplateRepository component

** Implementation Roadmap

*** Phase 1: Infrastructure
1. Create =TemplateRepository= interface and implementation
2. Modify =StaticAutoscaler= to include =TemplateRepository=
3. Update =RunOnce= flow to use template persistence

*** Phase 2: Processor Updates
1. Modify =CustomResourcesProcessor= interface to accept templates
2. Update =DraCustomResourcesProcessor= to use previous templates
3. Update any future CSI processors similarly

*** Phase 3: Testing and Validation
1. Unit tests for =TemplateRepository=
2. Integration tests for modified =RunOnce= flow
3. End-to-end tests for DRA scenarios
4. Performance testing to ensure no regressions

** Current Limitation Call-out

As noted in the DRA review feedback:

#+BEGIN_QUOTE
"This would not be an issue if the readiness logic operated on the final templates returned by TemplateNodeInfoProvider - which would come from real Nodes if possible, TemplateNodeInfo() if not. Unfortunately right now the final templates are just passed inside RunOnce() in a random variable."
#+END_QUOTE

The described solution directly addresses this limitation by:
1. **Extracting template calculation** from RunOnce() into a persistent component
2. **Making templates accessible** to both readiness logic and scaling logic
3. **Enabling the proper fallback sequence**: Real nodes → Cached real nodes → TemplateNodeInfo()

This refactor would fix both DRA and any future CSI implementations that face the same circular dependency issue.

* ScaleUp Function - Comprehensive Analysis

** High-Level Purpose

The =ScaleUp= function in =/home/hekumar/redhat/autoscaler/cluster-autoscaler/core/scaleup/orchestrator/orchestrator.go= (lines 85-284) orchestrates the entire scale-up operation. It analyzes unschedulable pods, evaluates which node groups can accommodate them, selects the best option, applies various limits and constraints, and executes the actual scale-up.

** Function Signature and Parameters

#+BEGIN_SRC go
func (o *ScaleUpOrchestrator) ScaleUp(
    unschedulablePods []*apiv1.Pod,      // Pods that couldn't be scheduled
    nodes []*apiv1.Node,                  // Current nodes in cluster
    daemonSets []*appsv1.DaemonSet,      // DaemonSets (for template calculation)
    nodeInfos map[string]*framework.NodeInfo,  // Template info per node group
    allOrNothing bool,                    // CRITICAL: all-or-nothing mode
) (*status.ScaleUpStatus, errors.AutoscalerError)
#+END_SRC

*** Critical Parameter: allOrNothing

This boolean fundamentally changes behavior:
- *=true=*: Either scale up enough capacity for ALL unschedulable pods, or don't scale at all
- *=false=*: Scale up for as many pods as possible, even if not all can be accommodated

** Phase 1: Initialization and Safety Checks

Lines 92-100:

#+BEGIN_SRC go
if !o.initialized {
    return status.UpdateScaleUpError(&status.ScaleUpStatus{},
        errors.NewAutoscalerError(errors.InternalError, "ScaleUpOrchestrator is not initialized"))
}
#+END_SRC

*Edge Case #1*: Prevents panics if =Initialize()= wasn't called.

*** Logging Quota System

Lines 96-100:

#+BEGIN_SRC go
loggingQuota := klogx.PodsLoggingQuota()
for _, pod := range unschedulablePods {
    klogx.V(1).UpTo(loggingQuota).Infof("Pod %s/%s is unschedulable", pod.Namespace, pod.Name)
}
klogx.V(1).Over(loggingQuota).Infof("%v other pods are also unschedulable", -loggingQuota.Left())
#+END_SRC

*Subtlety*: This prevents log flooding when there are hundreds/thousands of unschedulable pods. It logs details for the first N pods, then a summary for the rest.

** Phase 2: Pod Equivalence Grouping

Lines 102-104:

#+BEGIN_SRC go
podEquivalenceGroups := equivalence.BuildPodGroups(unschedulablePods)
#+END_SRC

*** Critical Optimization: Pod Grouping

Groups pods that are *semantically equivalent* for scheduling:
- Same controller (ReplicationController, Deployment, etc.)
- Same labels
- Semantically equal =PodSpec= (ignoring volume names, etc.)

*Why This Matters*:
- Instead of testing scheduling for 100 identical pods individually, test once and multiply results
- *Major performance improvement* for large replica sets

*Edge Case #2*: DaemonSet pods are *never grouped* - each gets its own group (line 69 in =groups.go=)

*Edge Case #3*: *Limit of 10 equivalence groups per controller* (line 58: =maxEquivalenceGroupsByController = 10=)
- Prevents explosion of groups when a controller has many different pod specs
- After 10 groups, remaining pods each get their own group

** Phase 3: Upcoming Nodes Check

Lines 106-110:

#+BEGIN_SRC go
upcomingNodes, aErr := o.UpcomingNodes(nodeInfos)
klog.V(4).Infof("Upcoming %d nodes", len(upcomingNodes))
#+END_SRC

*Subtlety*: "Upcoming nodes" are nodes that have been requested but not yet registered. This count is used to:
1. Prevent over-provisioning (don't scale up if nodes are already coming)
2. Calculate current cluster size for limit checks

** Phase 4: Node Group Filtering

Lines 112-119:

#+BEGIN_SRC go
if o.processors != nil && o.processors.NodeGroupListProcessor != nil {
    nodeGroups, nodeInfos, err = o.processors.NodeGroupListProcessor.Process(...)
}
#+END_SRC

*Edge Case #4*: *Processor can modify both* =nodeGroups= AND =nodeInfos=
- Allows dynamic filtering based on current cluster state
- Can inject additional node groups or remove some

** Phase 5: Resource Limits Calculation

Lines 121-127:

#+BEGIN_SRC go
o.processors.BinpackingLimiter.InitBinpacking(o.autoscalingContext, nodeGroups)
resourcesLeft, aErr := o.resourceManager.ResourcesLeft(o.autoscalingContext, nodeInfos, nodes)
#+END_SRC

*Important*: Calculates remaining capacity before hitting cluster-wide limits:
- Max total CPU
- Max total memory
- Max total GPU
- Custom resources (DRA, etc.)

*Edge Case #5*: This is calculated *before* considering node group limits, so both must be checked separately.

** Phase 6: Node Group Validation

Lines 131-137:

#+BEGIN_SRC go
validNodeGroups, skippedNodeGroups := o.filterValidScaleUpNodeGroups(
    nodeGroups, nodeInfos, resourcesLeft, len(nodes)+len(upcomingNodes), now)
#+END_SRC

*** Complex Filtering Logic

From lines 396-452, the filtering applies multiple checks:

**** Filter #1: Readiness

Line 408:

#+BEGIN_SRC go
if skipReason := o.IsNodeGroupReadyToScaleUp(nodeGroup, now); skipReason != nil {
    skippedNodeGroups[nodeGroup.Id()] = skipReason
    continue
}
#+END_SRC

- Checks backoff state (failed scale-ups)
- Checks if node group exists

**** Filter #2: Max Size Reached

Lines 419-423:

#+BEGIN_SRC go
if currentTargetSize >= nodeGroup.MaxSize() {
    skippedNodeGroups[nodeGroup.Id()] = MaxLimitReachedReason
    continue
}
#+END_SRC

**** Filter #3: ZeroOrMaxNodeScaling Special Case

*Edge Case #6*: *Atomic scaling mode* (lines 424-436)

#+BEGIN_SRC go
if autoscalingOptions != nil && autoscalingOptions.ZeroOrMaxNodeScaling {
    numNodes = nodeGroup.MaxSize() - currentTargetSize
    if currentNodeCount+numNodes > o.autoscalingContext.MaxNodesTotal {
        // Skip this node group
    }
}
#+END_SRC

*Critical Subtlety*: Some node groups (typically GPU) must scale from 0 to max in one shot. This checks if doing so would exceed cluster limits *before* attempting binpacking.

**** Filter #4: Resource Limits

Lines 444-447:

#+BEGIN_SRC go
if skipReason := o.IsNodeGroupResourceExceeded(resourcesLeft, nodeGroup, nodeInfo, numNodes); skipReason != nil {
    skippedNodeGroups[nodeGroup.Id()] = skipReason
    continue
}
#+END_SRC

*Edge Case #7*: Skipped node groups are tracked with *reasons* for observability - used in final status.

** Phase 7: Schedulable Pod Group Calculation

Lines 139-145:

#+BEGIN_SRC go
schedulablePodGroups := map[string][]estimator.PodEquivalenceGroup{}
for _, nodeGroup := range validNodeGroups {
    schedulablePodGroups[nodeGroup.Id()] = o.SchedulablePodGroups(
        podEquivalenceGroups, nodeGroup, nodeInfos[nodeGroup.Id()])
}
#+END_SRC

*Critical Operation*: For each node group, determines which pod groups can actually be scheduled there.

From lines 596-636, this:
1. *Forks* the cluster snapshot (transactional simulation!)
2. Adds a test node from the node group
3. Tries to schedule a representative pod from each equivalence group
4. *Reverts* the snapshot (cleanup)

*Edge Case #8*: Uses fork/revert to ensure simulation doesn't affect main cluster state.

** Phase 8: Expansion Options Computation

Lines 147-162:

#+BEGIN_SRC go
for _, nodeGroup := range validNodeGroups {
    option := o.ComputeExpansionOption(nodeGroup, schedulablePodGroups,
        nodeInfos, len(nodes)+len(upcomingNodes), now, allOrNothing)

    if len(option.Pods) == 0 || option.NodeCount == 0 {
        klog.V(4).Infof("No pod can fit to %s", nodeGroup.Id())
    } else if allOrNothing && len(option.Pods) < len(unschedulablePods) {
        klog.V(4).Infof("Some pods can't fit to %s, giving up due to all-or-nothing strategy",
            nodeGroup.Id())
    } else {
        options = append(options, option)
    }

    if o.processors.BinpackingLimiter.StopBinpacking(o.autoscalingContext, options) {
        break
    }
}
#+END_SRC

*** ComputeExpansionOption Details

Lines 455-513:
1. Finds "similar node groups" for potential balancing
2. Runs *binpacking estimator* to determine how many nodes are needed
3. Applies =ZeroOrMaxNodeScaling= logic if needed (lines 501-510)

*Edge Case #9*: *All-or-nothing filtering happens HERE* (lines 153-154)
- If =allOrNothing=true= and node group can't fit ALL pods, the option is discarded
- This is *before* expander selection

*Edge Case #10*: *Binpacking limiter can stop early* (line 159)
- Performance optimization: if we already found N good options, stop evaluating more node groups
- Prevents expensive binpacking calculations for all node groups

*Edge Case #11*: *ZeroOrMaxNodeScaling forces specific node count* (lines 501-510)

#+BEGIN_SRC go
if autoscalingOptions != nil && autoscalingOptions.ZeroOrMaxNodeScaling {
    if option.NodeCount > 0 {
        option.NodeCount = nodeGroup.MaxSize()  // Force to max!
    }
}
#+END_SRC

** Phase 9: Expander Selection

Lines 167-189:

#+BEGIN_SRC go
if len(options) == 0 {
    return &status.ScaleUpStatus{
        Result: status.ScaleUpNoOptionsAvailable,
        PodsRemainUnschedulable: GetRemainingPods(podEquivalenceGroups, skippedNodeGroups),
        ConsideredNodeGroups: nodeGroups,
    }, nil
}

bestOption := o.autoscalingContext.ExpanderStrategy.BestOption(options, nodeInfos)
if bestOption == nil || bestOption.NodeCount <= 0 {
    return &status.ScaleUpStatus{
        Result: status.ScaleUpNoOptionsAvailable,
        ...
    }, nil
}
#+END_SRC

*Edge Case #12*: *Two early exits* for no options:
1. No options generated at all
2. Expander returned nil or invalid option

*Subtlety*: Different expanders (random, most-pods, least-waste, price, priority) can return different results for the same options.

** Phase 10: Node Count Capping

*** Cap #1: Cluster-wide node count

Lines 192-195:

#+BEGIN_SRC go
newNodes, aErr := o.GetCappedNewNodeCount(bestOption.NodeCount, len(nodes)+len(upcomingNodes))
#+END_SRC

*** Cap #2: Resource limits

Lines 197-202:

#+BEGIN_SRC go
newNodes, aErr = o.applyLimits(newNodes, resourcesLeft, bestOption.NodeGroup, nodeInfos)
#+END_SRC

*** Critical All-or-Nothing Check #1

Lines 204-212:

#+BEGIN_SRC go
if newNodes < bestOption.NodeCount {
    klog.V(1).Infof("Only %d nodes can be added to %s due to cluster-wide limits",
        newNodes, bestOption.NodeGroup.Id())
    if allOrNothing {
        klog.V(1).Info("Not attempting scale-up due to all-or-nothing strategy: not all pods would be accommodated")
        markedEquivalenceGroups := markAllGroupsAsUnschedulable(podEquivalenceGroups, AllOrNothingReason)
        return buildNoOptionsAvailableStatus(markedEquivalenceGroups, skippedNodeGroups, nodeGroups), nil
    }
}
#+END_SRC

*Edge Case #13*: If capping reduced node count and =allOrNothing=true=, *abort the entire scale-up*.

** Phase 11: Node Group Creation (If Needed)

Lines 214-236:

#+BEGIN_SRC go
if !bestOption.NodeGroup.Exist() && !o.processors.AsyncNodeGroupStateChecker.IsUpcoming(bestOption.NodeGroup) {
    if allOrNothing && bestOption.NodeGroup.MaxSize() < newNodes {
        // Can't create a node group big enough
        return buildNoOptionsAvailableStatus(...)
    }

    if o.autoscalingContext.AsyncNodeGroupsEnabled {
        createNodeGroupResults, scaleUpStatus, aErr = o.CreateNodeGroupAsync(...)
    } else {
        createNodeGroupResults, scaleUpStatus, aErr = o.CreateNodeGroup(...)
    }
}
#+END_SRC

*Edge Case #14*: *Node group might not exist yet!*
- Autoprovisioning creates node groups dynamically
- Must create before scaling

*** Critical All-or-Nothing Check #2

Lines 217-223:

#+BEGIN_SRC go
if allOrNothing && bestOption.NodeGroup.MaxSize() < newNodes {
    // New node group can't be created with enough capacity
    return buildNoOptionsAvailableStatus(...)
}
#+END_SRC

*Edge Case #15*: *Async vs. Sync node group creation*
- Async: Returns immediately, node group creation happens in background
- Sync: Waits for cloud provider to confirm creation

*Subtlety*: After creation, *node info is regenerated* (lines 561-585) using the actual created node group, not the expansion candidate. This ensures balancing works correctly.

** Phase 12: Balance Scale-Ups

Lines 238-243:

#+BEGIN_SRC go
scaleUpInfos, aErr := o.balanceScaleUps(now, bestOption.NodeGroup, newNodes, nodeInfos, schedulablePodGroups)
#+END_SRC

*Edge Case #16*: *Similar node group balancing* (lines 709-744)
- If multiple node groups are "similar" (same capacity, can schedule same pods), CA distributes new nodes across them
- Ensures even distribution for high availability
- Only happens if =BalanceSimilarNodeGroups= flag is enabled

*Example*: Need 10 nodes, have 3 similar node groups → might add 4+3+3 instead of 10+0+0

** Phase 13: Final Capacity Check

Lines 245-258:

#+BEGIN_SRC go
totalCapacity := 0
for _, sui := range scaleUpInfos {
    totalCapacity += sui.NewSize - sui.CurrentSize
}
if totalCapacity < newNodes {
    if allOrNothing {
        // Can't accommodate all pods
        return buildNoOptionsAvailableStatus(...)
    }
}
#+END_SRC

*** Critical All-or-Nothing Check #3

Lines 252-257.

*Edge Case #17*: *Balancing might reduce capacity!*
- Balancing spreads nodes across groups, but some groups might be at/near max size
- Total capacity after balancing might be less than requested
- If =allOrNothing=true=, this causes abort

*Subtlety*: This is the *LAST* check before actual execution. Three all-or-nothing checks ensure no partial scale-ups slip through.

** Phase 14: Execution

Lines 260-272:

#+BEGIN_SRC go
klog.V(1).Infof("Final scale-up plan: %v", scaleUpInfos)
aErr, failedNodeGroups := o.scaleUpExecutor.ExecuteScaleUps(scaleUpInfos, nodeInfos, now, allOrNothing)
#+END_SRC

*This is where actual cloud provider calls happen!*
- Calls =nodeGroup.IncreaseSize()=
- Updates cluster state registry
- Applies taints to new nodes (if configured)

*Edge Case #18*: *Execution can partially fail*
- Some node groups might scale successfully, others fail
- =failedNodeGroups= tracks which ones failed
- Even on failure, successful scale-ups are reflected in returned status

** Phase 15: Success Path

Lines 274-283:

#+BEGIN_SRC go
o.clusterStateRegistry.Recalculate()
return &status.ScaleUpStatus{
    Result: status.ScaleUpSuccessful,
    ScaleUpInfos: scaleUpInfos,
    PodsRemainUnschedulable: GetRemainingPods(podEquivalenceGroups, skippedNodeGroups),
    ConsideredNodeGroups: nodeGroups,
    CreateNodeGroupResults: createNodeGroupResults,
    PodsTriggeredScaleUp: bestOption.Pods,
    PodsAwaitEvaluation: GetPodsAwaitingEvaluation(podEquivalenceGroups, bestOption.NodeGroup.Id()),
}
#+END_SRC

*Subtlety*: *Success doesn't mean ALL pods scheduled!*
- =PodsRemainUnschedulable=: Pods that couldn't fit anywhere
- =PodsTriggeredScaleUp=: Pods that caused this scale-up
- =PodsAwaitEvaluation=: Pods that might be scheduled once new nodes are ready

** Summary of Critical Edge Cases

The =ScaleUp= function demonstrates masterful defensive programming with 18 identified edge cases:

1. ✅ Uninitialized orchestrator check
2. ✅ DaemonSet pods never grouped
3. ✅ 10 group limit per controller
4. ✅ Processor can modify node groups
5. ✅ Resource limits checked separately from node limits
6. ✅ ZeroOrMaxNodeScaling atomic mode
7. ✅ Skipped node groups tracked with reasons
8. ✅ Fork/revert for safe scheduling simulation
9. ✅ All-or-nothing filtering before expander
10. ✅ Binpacking limiter can stop early
11. ✅ ZeroOrMaxNodeScaling forces specific count
12. ✅ Two early exits for no options
13. ✅ *THREE all-or-nothing checks* (caps, creation, balancing)
14. ✅ Non-existent node groups must be created first
15. ✅ Async vs. sync creation modes
16. ✅ Similar node group balancing
17. ✅ Balancing can reduce total capacity
18. ✅ Partial execution failures possible

** Key Design Patterns

*** Defense in Depth
Multiple validation layers ensure no invalid scale-ups occur:
- Pre-filtering node groups
- Binpacking simulation
- Expander selection
- Multiple limit checks
- Three separate all-or-nothing validations

*** Transactional Simulation
Fork/revert pattern ensures all "what-if" analysis is safe and doesn't corrupt cluster state.

*** Performance Optimization
- Pod equivalence grouping
- Early termination via binpacking limiter
- Lazy evaluation where possible

*** Observability
- Detailed status tracking
- Reason codes for skipped node groups
- Comprehensive metrics and logging

#+BEGIN_QUOTE
*Note*: This analysis is based on the Kubernetes Cluster Autoscaler codebase as of the examination date. Implementation details may vary in different versions.
#+END_QUOTE
